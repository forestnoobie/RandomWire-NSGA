data:
  train: '' # if using MNIST, can download directly from cloud with train: 'download', test: 'download'
  val: ''
  batch_size: 256 # learning rate should be adjusted along with batch_size. see 1706.02677
  num_workers: 16
  type: '' # MNIST, CIFAR10, ImageNet
---
train:
  optimizer: 'adam'
  epoch: 250 # 'small regime'. see table 2 of the paper.
  adam: 0.001
  adabound:
    initial: 0.001
    final: 0.05
  sgd:
    lr: 0.1
    momentum: 0.9
    weight_decay: 0.00005
  decay:
    step: 150000
    gamma: 0.1
---
model:
  channel: 78 # 'small regime'
  classes: 1000 # use 10 for MNIST, CIFAR10
  input_maps: 3 # 3 for ImageNet/CIFAR10, 1 for MNIST
  graph0: '' # example: 'ws-4-075-3.txt'
  graph1: ''
  graph2: ''
---
log:
  chkpt_dir: 'chkpt'
  log_dir: 'logs'
