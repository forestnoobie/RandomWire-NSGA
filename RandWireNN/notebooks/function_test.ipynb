{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ga trainer\n",
    "* read graph from chromosome\n",
    "* checkpoint 수정\n",
    "* termination criteria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "from utils.train import train\n",
    "from utils.hparams import HParam\n",
    "from utils.writer import MyWriter\n",
    "from utils.graph_reader import read_graph\n",
    "from dataset.dataloader import create_dataloader, MNIST_dataloader, CIFAR10_dataloader\n",
    "\n",
    "\n",
    "# 원래는 yaml 파일, checkpoint path, name of model args로 넘기자\n",
    "## parser만 잘 바꾸면 될듯\n",
    "def ga_trainer(args,index_list,f_path,f_name):\n",
    "    \n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-c', '--config', type=str, required=True,\n",
    "#                         help=\"yaml file for configuration\")\n",
    "#     parser.add_argument('-p', '--checkpoint_path', type=str, default=None, required=False,\n",
    "#                         help=\"path of checkpoint pt file\")\n",
    "#     parser.add_argument('-m', '--model', type=str, required=True,\n",
    "#                         help=\"name of the model. used for logging/saving checkpoints\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    individual_model_name =args.model + \"_{}_{}_{}\".format(index_list[0],index_list[1],\n",
    "                                                          index_list[2])\n",
    "    \n",
    "    hp = HParam(args.config)\n",
    "    with open(args.config, 'r') as f:\n",
    "        hp_str = ''.join(f.readlines())\n",
    "    ## pytoch 모델 저장하는 위치\n",
    "    \n",
    "    pt_path = os.path.join('.', hp.log.chkpt_dir)\n",
    "    ## 모델 사전에 정의한 모델 이름으로 저장\n",
    "    out_dir = os.path.join(pt_path, individual_model_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    log_dir = os.path.join('.', hp.log.log_dir)\n",
    "    log_dir = os.path.join(log_dir, individual_model_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    if args.checkpoint_path is not None:\n",
    "        chkpt_path = args.checkpoint_path\n",
    "    else:\n",
    "        chkpt_path = None\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(os.path.join(log_dir,\n",
    "                '%s-%d.log' % (args.model, time.time()))),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    if hp.data.train == '' or hp.data.val == '':\n",
    "        logger.error(\"hp.data.train, hp.data.val cannot be empty\")\n",
    "        raise Exception(\"Please specify directories of train data.\")\n",
    "\n",
    "    if hp.model.graph0 == '' or hp.model.graph1 == '' or hp.model.graph2 == '':\n",
    "        logger.error(\"hp.model.graph0, graph1, graph2 cannot be empty\")\n",
    "        raise Exception(\"Please specify random DAG architecture.\")\n",
    "\n",
    "#     graphs = [\n",
    "#         read_graph(hp.model.graph0),\n",
    "#         read_graph(hp.model.graph1),\n",
    "#         read_graph(hp.model.graph2),\n",
    "#     ]\n",
    "\n",
    "    ## 새로 생성한 파일 위치에서 그래프 읽기\n",
    "    graphs = [read_graph(os.path.join(f_path,args.model + '_' + str(idx)+ '.txt')) for idx in index_list]\n",
    "\n",
    "    writer = MyWriter(log_dir)\n",
    "    \n",
    "    dataset = hp.data.type\n",
    "    switcher = {\n",
    "            'MNIST': MNIST_dataloader,\n",
    "            'CIFAR10':CIFAR10_dataloader,\n",
    "            'ImageNet':create_dataloader,\n",
    "            }\n",
    "    assert dataset in switcher.keys(), 'Dataset type currently not supported'\n",
    "    dl_func = switcher[dataset]\n",
    "    trainset = dl_func(hp, args, True)\n",
    "    valset = dl_func(hp, args, False)\n",
    "\n",
    "    val_acc = ga_train(out_dir, chkpt_path, trainset, valset, writer, logger, hp, hp_str, graphs)\n",
    "## Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 14:59:09,903 - INFO - Starting new training run\n",
      "2019-07-12 14:59:09,906 - INFO - Writing graph to tensorboardX...\n",
      "2019-07-12 14:59:13,215 - INFO - Finished.\n",
      "Loss 0.13 at step 58: 100%|████████████████████████████████████████████████████████████| 59/59 [00:35<00:00,  1.77it/s]\n",
      "78it [00:06, 12.37it/s]\n",
      "2019-07-12 14:59:55,357 - INFO - Saved checkpoint to: .\\chkpt\\ws-4-0.75_1_2_3\\chkpt_000.pt\n",
      "2019-07-12 14:59:55,359 - INFO - Validation Accuracy imporved: 0.488600\n",
      "Loss 0.08 at step 117: 100%|███████████████████████████████████████████████████████████| 59/59 [00:35<00:00,  1.78it/s]\n",
      "78it [00:06, 12.88it/s]\n",
      "2019-07-12 15:00:37,195 - INFO - Saved checkpoint to: .\\chkpt\\ws-4-0.75_1_2_3\\chkpt_001.pt\n",
      "2019-07-12 15:00:37,196 - INFO - Validation Accuracy imporved: 0.974000\n",
      "Loss 0.04 at step 176: 100%|███████████████████████████████████████████████████████████| 59/59 [00:35<00:00,  1.71it/s]\n",
      "78it [00:06, 12.31it/s]\n",
      "2019-07-12 15:01:19,156 - INFO - Saved checkpoint to: .\\chkpt\\ws-4-0.75_1_2_3\\chkpt_002.pt\n",
      "2019-07-12 15:01:19,157 - INFO - Validation Accuracy imporved: 0.977500\n",
      "Loss 0.04 at step 235: 100%|███████████████████████████████████████████████████████████| 59/59 [00:35<00:00,  1.80it/s]\n",
      "78it [00:06, 12.79it/s]\n",
      "Loss 0.02 at step 294: 100%|███████████████████████████████████████████████████████████| 59/59 [00:35<00:00,  1.78it/s]\n",
      "78it [00:06, 12.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#def ga_trainer(args,index_list,f_path,f_name):\n",
    "\n",
    "## 테스트성공\n",
    "if __name__ == '__main__':\n",
    "    args = easydict.EasyDict({\n",
    "  parser.add_argument('-m', '--model', type=str, required=True,\n",
    "#                         help=\"name of the model. used for logging/saving checkpoints\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    individual_model_name =args.model + \"_{}_{}_{}\".format(index_list[0],index_list[1],\n",
    "                                                          index_list[2])\n",
    "    \n",
    "    hp = HParam(args.config)\n",
    "    with open(args.config, 'r') as f:\n",
    "        hp_str = ''.join(f.readlines())\n",
    "    ## pytoch 모델 저장하는 위치\n",
    "    \n",
    "    pt_path = os.path.join('.', hp.log.chkpt_dir)\n",
    "    ## 모델 사전에 정의한 모델 이름으로 저장\n",
    "    out_dir = os.path.join(pt_path, individual_model_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    log_dir = os.path.join('.', hp.log.log_dir)\n",
    "    log_dir = os.path.join(log_dir, individual_model_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    if args.checkpoint_path is not None:\n",
    "        chkpt_path = args.checkpoint_path\n",
    "    else:\n",
    "        chkpt_path = None\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(os.path.join(log_dir,\n",
    "                '%s-%d.log' % (args.model, time.time()))),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    if hp.data.train == '' or hp.data.val == '':\n",
    "        logger.error(\"hp.data.train, hp.data.val cannot be empty\")\n",
    "        raise Exception(\"Please specify directories of train data.\")\n",
    "\n",
    "    if hp.model.graph0 == '' or hp.model.graph1 == '' or hp.model.graph2 == '':\n",
    "        logger.error(\"hp.model.graph0, graph1, graph2 cannot be empty\")\n",
    "        raise Exception(\"Please specify random DAG architecture.\")\n",
    "\n",
    "#     graphs = [\n",
    "#         read_graph(hp.model.graph0),\n",
    "#         read_graph(hp.model.graph1),\n",
    "#         read_graph(hp.model.graph2),\n",
    "#     ]\n",
    "\n",
    "    ## 새로 생성한 파일 위치에서 그래프 읽기\n",
    "    graphs = [read_graph(os.path.join(f_path,args.model + '_' + str(idx)+ '.txt')) for idx in index_list]\n",
    "\n",
    "    writer = MyWriter(log_dir)\n",
    "    \n",
    "    dataset = hp.data.type\n",
    "    switcher = {\n",
    "            'MNIST': MNIST_dataloader,\n",
    "            'CIFAR10':CIFAR10_dataloader,\n",
    "            'ImageNet':create_dataloader,\n",
    "            }\n",
    "    assert dataset in switcher.keys(), 'Dataset type currently not supported'\n",
    "    dl_func = switcher[dataset]\n",
    "    trainset = dl_func(hp, args, True)\n",
    "    valset = dl_func(hp, args, False)\n",
    "\n",
    "    val_acc = ga_train(out_dir, chkpt_path, trainset, valset, writer, logger, hp, hp_str, graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GA Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import adabound\n",
    "import itertools\n",
    "import traceback\n",
    "from torchsummary import summary\n",
    "\n",
    "from utils.hparams import load_hparam_str\n",
    "from utils.evaluation import validate\n",
    "from model.model import RandWire\n",
    "\n",
    "\n",
    "## GA train\\\n",
    "def ga_train(out_dir, chkpt_path, trainset, valset, writer, logger, hp, hp_str, graphs):\n",
    "    model = RandWire(hp, graphs).cuda()\n",
    "    if hp.train.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=hp.train.adam)\n",
    "    elif hp.train.optimizer == 'adabound':\n",
    "        optimizer = adabound.AdaBound(model.parameters(),\n",
    "                             lr=hp.train.adabound.initial,\n",
    "                             final_lr=hp.train.adabound.final)\n",
    "    elif hp.train.optimizer == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                    lr=hp.train.sgd.lr,\n",
    "                                    momentum=hp.train.sgd.momentum,\n",
    "                                    weight_decay=hp.train.sgd.weight_decay)\n",
    "    else:\n",
    "        raise Exception(\"Optimizer not supported: %s\" % hp.train.optimizer)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, hp.train.epoch)\n",
    "\n",
    "    init_epoch = -1\n",
    "    step = 0\n",
    "\n",
    "    if chkpt_path is not None:\n",
    "        logger.info(\"Resuming from checkpoint: %s\" % chkpt_path)\n",
    "        checkpoint = torch.load(chkpt_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        step = checkpoint['step']\n",
    "        init_epoch = checkpoint['epoch']\n",
    "\n",
    "        if hp_str != checkpoint['hp_str']:\n",
    "            logger.warning(\"New hparams are different from checkpoint.\")\n",
    "            logger.warning(\"Will use new hparams.\")\n",
    "        # hp = load_hparam_str(hp_str)\n",
    "    else:\n",
    "        logger.info(\"Starting new training run\")\n",
    "        logger.info(\"Writing graph to tensorboardX...\")\n",
    "        writer.write_graph(model, torch.randn(7, hp.model.input_maps, 224, 224).cuda())\n",
    "        logger.info(\"Finished.\")\n",
    "\n",
    "    try:\n",
    "        model.train()\n",
    "        patients = 0\n",
    "        prev_acc = 0\n",
    "        for epoch in itertools.count(init_epoch+1):\n",
    "            loader = tqdm.tqdm(trainset, desc='Train data loader')\n",
    "            for data, target in loader:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.nll_loss(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss = loss.item()\n",
    "                if loss > 1e8 or math.isnan(loss):\n",
    "                    logger.error(\"Loss exploded to %.02f at step %d!\" % (loss, step))\n",
    "                    raise Exception(\"Loss exploded\")\n",
    "\n",
    "                writer.log_training(loss, step)\n",
    "                loader.set_description('Loss %.02f at step %d' % (loss, step))\n",
    "                step += 1\n",
    "            \n",
    "            #validation\n",
    "            val_loss, val_acc = validate(model, valset, writer, epoch)\n",
    "            \n",
    "            if prev_acc < val_acc:\n",
    "                save_path = os.path.join(out_dir, 'chkpt_%03d.pt' % epoch)\n",
    "                torch.save({\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                    'step': step,\n",
    "                    'epoch': epoch,\n",
    "                    'hp_str': hp_str,\n",
    "                }, save_path)\n",
    "                logger.info(\"Saved checkpoint to: %s\" % save_path)\n",
    "                logger.info(\"Validation Accuracy imporved: %3f\" % val_acc)\n",
    "\n",
    "                patients = 0\n",
    "                prev_acc = val_acc\n",
    "            else :\n",
    "                patients += 1\n",
    "                \n",
    "            if patients > 5 :\n",
    "                break\n",
    "                \n",
    "            ## 임시 조건\n",
    "            if epoch > 3:\n",
    "                break\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.info(\"Exiting due to exception: %s\" % e)\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    return prev_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import adabound\n",
    "import itertools\n",
    "import traceback\n",
    "from torchsummary import summary\n",
    "\n",
    "from utils.hparams import load_hparam_str\n",
    "from utils.evaluation import validate\n",
    "from model.model import RandWire\n",
    "\n",
    "\n",
    "def train(out_dir, chkpt_path, trainset, valset, writer, logger, hp, hp_str, graphs):\n",
    "    model = RandWire(hp, graphs).cuda()\n",
    "    if hp.train.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=hp.train.adam)\n",
    "    elif hp.train.optimizer == 'adabound':\n",
    "        optimizer = adabound.AdaBound(model.parameters(),\n",
    "                             lr=hp.train.adabound.initial,\n",
    "                             final_lr=hp.train.adabound.final)\n",
    "    elif hp.train.optimizer == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                    lr=hp.train.sgd.lr,\n",
    "                                    momentum=hp.train.sgd.momentum,\n",
    "                                    weight_decay=hp.train.sgd.weight_decay)\n",
    "    else:\n",
    "        raise Exception(\"Optimizer not supported: %s\" % hp.train.optimizer)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, hp.train.epoch)\n",
    "\n",
    "    init_epoch = -1\n",
    "    step = 0\n",
    "\n",
    "    if chkpt_path is not None:\n",
    "        logger.info(\"Resuming from checkpoint: %s\" % chkpt_path)\n",
    "        checkpoint = torch.load(chkpt_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        step = checkpoint['step']\n",
    "        init_epoch = checkpoint['epoch']\n",
    "\n",
    "        if hp_str != checkpoint['hp_str']:\n",
    "            logger.warning(\"New hparams are different from checkpoint.\")\n",
    "            logger.warning(\"Will use new hparams.\")\n",
    "        # hp = load_hparam_str(hp_str)\n",
    "    else:\n",
    "        logger.info(\"Starting new training run\")\n",
    "        logger.info(\"Writing graph to tensorboardX...\")\n",
    "        #print(model)\n",
    "        # parameters = 0\n",
    "        # for p in list(model.parameters()):\n",
    "        #     nn =1\n",
    "        #     for s in list(p.size()):\n",
    "        #         nn = nn * s\n",
    "        #     parameters += nn\n",
    "        #print(\"Parameters\",parameters)\n",
    "\n",
    "        #print(\"model\",hp.model)\n",
    " #       summary(model,(1,224,224))\n",
    "        writer.write_graph(model, torch.randn(7, hp.model.input_maps, 224, 224).cuda())\n",
    "        #Batch??, input_channels, width,depths\n",
    "        logger.info(\"Finished.\")\n",
    "\n",
    "    try:\n",
    "        model.train()\n",
    "        for epoch in itertools.count(init_epoch+1):\n",
    "            loader = tqdm.tqdm(trainset, desc='Train data loader')\n",
    "            for data, target in loader:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.nll_loss(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss = loss.item()\n",
    "                if loss > 1e8 or math.isnan(loss):\n",
    "                    logger.error(\"Loss exploded to %.02f at step %d!\" % (loss, step))\n",
    "                    raise Exception(\"Loss exploded\")\n",
    "\n",
    "                writer.log_training(loss, step)\n",
    "                loader.set_description('Loss %.02f at step %d' % (loss, step))\n",
    "                step += 1                \n",
    "\n",
    "            save_path = os.path.join(out_dir, 'chkpt_%03d.pt' % epoch)\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'step': step,\n",
    "                'epoch': epoch,\n",
    "                'hp_str': hp_str,\n",
    "            }, save_path)\n",
    "            logger.info(\"Saved checkpoint to: %s\" % save_path)\n",
    "\n",
    "            validate(model, valset, writer, epoch)\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.info(\"Exiting due to exception: %s\" % e)\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph making code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser('Erdos-Renyi graph generator')\n",
    "    parser.add_argument('-n', '--n_nodes', type=int, default=32,\n",
    "                        help=\"number of nodes for random graph\")\n",
    "    parser.add_argument('-k', '--k_neighbors', type=int, required=True,\n",
    "                        help=\"connecting neighboring nodes for WS\")\n",
    "    parser.add_argument('-p', '--prob', type=float, required=True,\n",
    "                        help=\"probablity of rewiring for WS\")\n",
    "    parser.add_argument('-o', '--out_txt', type=str, required=True,\n",
    "                        help=\"name of output txt file\")\n",
    "\n",
    "    parser.add_argument('-f', '--file_num', type=int, required=True, default=1,\n",
    "                        help=\"number of files to generate\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    n, k, p, file_num = args.n_nodes, args.k_neighbors, args.prob, args.file_num\n",
    "\n",
    "    assert k % 2 == 0, \"k must be even.\"\n",
    "    assert 0 < k < n, \"k must be larger than 0 and smaller than n.\"\n",
    "\n",
    "    os.makedirs('ga_initials', exist_ok=True)\n",
    "    for num in range(file_num):\n",
    "        adj = [[False] * n for _ in range(n)]  # adjacency matrix\n",
    "        for i in range(n):\n",
    "            adj[i][i] = True\n",
    "\n",
    "        # initial connection\n",
    "        for i in range(n):\n",
    "            for j in range(i - k // 2, i + k // 2 + 1):\n",
    "                real_j = j % n\n",
    "                if real_j == i:\n",
    "                    continue\n",
    "                adj[real_j][i] = adj[i][real_j] = True\n",
    "\n",
    "        rand = np.random.uniform(0.0, 1.0, size=(n, k // 2))\n",
    "        for i in range(n):\n",
    "            for j in range(1, k // 2 + 1):  # 'j' here is 'i' of paper's notation\n",
    "                current = (i + j) % n\n",
    "                if rand[i][j - 1] < p:  # rewire\n",
    "                    unoccupied = [x for x in range(n) if not adj[i][x]]\n",
    "                    rewired = np.random.choice(unoccupied)\n",
    "                    adj[i][current] = adj[current][i] = False\n",
    "                    adj[i][rewired] = adj[rewired][i] = True\n",
    "\n",
    "        edges = list()\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                if adj[i][j]:\n",
    "                    edges.append((i, j))\n",
    "\n",
    "        edges.sort()\n",
    "\n",
    "        file_name = args.out_txt[:-4] + \"_\" + str(num) + \".txt\"\n",
    "        with open(os.path.join('ga_initials', file_name), 'w') as f:\n",
    "            f.write(str(n) + '\\n')\n",
    "            f.write(str(len(edges)) + '\\n')\n",
    "            for edge in edges:\n",
    "                f.write('%d %d\\n' % (edge[0], edge[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'newrw'",
   "language": "python",
   "name": "newrw"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
