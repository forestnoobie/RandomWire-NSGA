{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWNN_JiaminRen 기준으로 zero base(빈 폴더)에서 구축\n",
    "\n",
    "- 실험환경: 1 gpu (Geforce 1080ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn    # for hardware tunning (cudnn.benchmark = True)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from model import CNN\n",
    "from lr_scheduler import LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN:\n",
    "#     workers: 3\n",
    "#     batch_size: 256\n",
    "#     epochs: 100\n",
    "\n",
    "#     lr_mode : cosine\n",
    "#     base_lr: 0.1\n",
    "#     warmup_epochs: 0\n",
    "#     warmup_lr: 0.0\n",
    "#     targetlr : 0.0\n",
    "\n",
    "#     momentum: 0.9\n",
    "#     weight_decay: 0.00005\n",
    "\n",
    "#     net_type : regular\n",
    "#     channels : 109\n",
    "#     nodes : 32\n",
    "#     graph_model : WS\n",
    "#     K : 4\n",
    "#     P : 0.75\n",
    "#     seed : 1\n",
    "\n",
    "#     print_freq: 100\n",
    "#     model_dir: checkpoint/regular_c109_n32\n",
    "\n",
    "#     train_root: data/imagenet/train\n",
    "#     train_source: data/imagenet/train.txt\n",
    "#     val_root: data/imagenet/val\n",
    "#     val_source: data/imagenet/val.txt\n",
    "\n",
    "# TEST:\n",
    "#     checkpoint_path : data/pretrained_model/regular_c109_n32.pth\n",
    "\n",
    "\n",
    "#########################################\n",
    "# LR schedular() 에 쓰이는 것들  // 없는 것 : args.step, args.decay_factor, args.power\n",
    "# args.lr_mode, args.warmup_mode, args.base_lr\n",
    "# args.targetlr, args.warmup_lr, args.epochs, args.warmup_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# 1. Configuration\n",
    "########################### \n",
    "class Args():\n",
    "    def __init__(self, run_code='default_run_code'):\n",
    "        self.run_code = run_code\n",
    "        self.workers = 2\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 100\n",
    "\n",
    "        self.lr_mode = 'cosine'\n",
    "        self.warmup_mode = 'linear'    # default\n",
    "        self.base_lr = 0.1\n",
    "        \n",
    "        self.warmup_epochs = 0\n",
    "        self.warmup_lr = 0.0\n",
    "        self.targetlr = 0.0\n",
    "\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 0.00005\n",
    "\n",
    "        self.net_type = 'regular'\n",
    "        self.channels = 109\n",
    "        self.nodes = 32\n",
    "        self.graph_model = 'WS'\n",
    "        self.K = 4\n",
    "        self.P = 0.75\n",
    "        \n",
    "        self.resume = False    # checkpoint에 저장된 모델 불러와서 사용할지 여부\n",
    "\n",
    "        self.seed = 1\n",
    "\n",
    "        self.print_freq = 100\n",
    "        \n",
    "        self.model_dir = './checkpoint' + '/' + run_code + '/'    # graph 정보 (e.g. conv2.yaml 등), \n",
    "        self.tensorboard_path = './tensorboard/' + run_code + '/'\n",
    "        self.log_path = './logs/' + run_code + '/'\n",
    "\n",
    "#         self.model_name = temp_date_str + '_model_'\n",
    "\n",
    "        if not os.path.isdir(self.model_dir): os.makedirs(self.model_dir)\n",
    "        if not os.path.isdir(self.tensorboard_path): os.makedirs(self.tensorboard_path)\n",
    "        if not os.path.isdir(self.log_path): os.makedirs(self.log_path)        \n",
    "\n",
    "        logging.basicConfig(filename=self.log_path + 'logging.log', level=logging.INFO)\n",
    "        logging.info('Configuration initialized.')\n",
    "        \n",
    "        # train_root: data/imagenet/train\n",
    "        # train_source: data/imagenet/train.txt\n",
    "        # val_root: data/imagenet/val\n",
    "        # val_source: data/imagenet/val.txt\n",
    "\n",
    "#         self.checkpoint_path = 'data/pretrained_model/regular_c109_n32.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint/regular_c109_n32/\n"
     ]
    }
   ],
   "source": [
    "args = Args(run_code='regular_c109_n32')    # default source\n",
    "print(args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# 2. Model, scheduler, loss, optimizer\n",
    "###########################\n",
    "model = CNN(args)\n",
    "\n",
    "model = nn.DataParallel(model)  # for multi-GPU\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "logging.info('Model is initialized')\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.base_lr,\n",
    "                            momentum=args.momentum,\n",
    "                            weight_decay=args.weight_decay)\n",
    "\n",
    "######### 일단, args.evaluate는 무시하고 구현 ###########################################\n",
    "# if args.evaluate:\n",
    "#     # evaluation mode 였다면, validate 하고 'return' 하며 끝남.\n",
    "#     print(\"args.evaluate: \", args.evaluate)\n",
    "#     validate(val_loader, model, criterion, 0, writer)\n",
    "#     # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# 3. saved model이 있다면 해당 checkpoint를 load. 아니면, train from scratch\n",
    "####################################################\n",
    "# auto resume from a checkpoint\n",
    "# start_epoch = 0\n",
    "# if args.evaluate:\n",
    "#     load_state_ckpt(args.checkpoint_path, model)\n",
    "# else:\n",
    "#     best_prec1, start_epoch = load_state(model_dir, model, optimizer=optimizer)\n",
    "# if args.rank == 0:\n",
    "#     writer = SummaryWriter(model_dir)\n",
    "# else:\n",
    "#     writer = None\n",
    "\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "start_epoch  = 0\n",
    "best_prec1 = 0\n",
    "\n",
    "# write tensorboard summaries & logger\n",
    "writer = SummaryWriter(log_dir=args.tensorboard_path)\n",
    "\n",
    "cudnn.benchmark = True    # This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# 4. Dataset & Dataloader\n",
    "###########################\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),   # 추가함\n",
    "        transforms.Resize(224),    # 추가함.  imagenet dataset과 size 맞추기\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # rescale 0 ~ 1 => -1 ~ 1\n",
    "    ])\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),    # 추가함.  imagenet dataset과 size 맞추기\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # rescale 0 ~ 1 => -1 ~ 1\n",
    "    ])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=val_transform)\n",
    "\n",
    "# sampler를 생략함\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                          shuffle=True, num_workers=args.workers)  \n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size,\n",
    "                                         shuffle=False, num_workers=args.workers)\n",
    "\n",
    "logging.info('Dataset & DataLoader is ready')\n",
    "\n",
    "####################### evaluate 가 왜 필요하지?\n",
    "# if args.evaluate:\n",
    "#     validate(val_loader, model, criterion, 0, writer)\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    # torch.topk : input, k, dim=None, largest=True, sorted=True => returns top k element\n",
    "    # returns values list & indices list\n",
    "    _, pred = output.topk(maxk, 1, True, True)    \n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))   # torch.eq: Computes element-wise equality\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)   # input, dim,\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, lr_scheduler, epoch, writer):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        lr_scheduler.update(i, epoch)\n",
    "        \n",
    "        target = target.cuda(async=True)\n",
    "        \n",
    "        input_var = torch.autograd.Variable(input.cuda())\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "        \n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "            \n",
    "            logging.info('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "            \n",
    "            niter = epoch * len(train_loader) + i\n",
    "            writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], niter)\n",
    "            writer.add_scalar('Train/Avg_Loss', losses.avg, niter)\n",
    "            writer.add_scalar('Train/Avg_Top1', top1.avg / 100.0, niter)\n",
    "            writer.add_scalar('Train/Avg_Top5', top5.avg / 100.0, niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, writer):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda(async=True)\n",
    "            input_var = torch.autograd.Variable(input.cuda(), volatile=True)\n",
    "            target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "            top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                    i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                    top1=top1, top5=top5))\n",
    "                \n",
    "                logging.info('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                    i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                    top1=top1, top5=top5))                \n",
    "                \n",
    "\n",
    "        print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "        niter = (epoch + 1)\n",
    "        writer.add_scalar('Eval/Avg_Loss', losses.avg, niter)\n",
    "        writer.add_scalar('Eval/Avg_Top1', top1.avg / 100.0, niter)\n",
    "        writer.add_scalar('Eval/Avg_Top5', top5.avg / 100.0, niter)\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 11.467 (11.467)\tData 0.401 (0.401)\tLoss 7.0085 (7.0085)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Epoch: [0][100/391]\tTime 0.635 (0.777)\tData 0.009 (0.013)\tLoss 1.6835 (2.1347)\tPrec@1 42.188 (25.766)\tPrec@5 88.281 (78.473)\n",
      "Epoch: [0][200/391]\tTime 0.661 (0.728)\tData 0.009 (0.011)\tLoss 1.7369 (1.9198)\tPrec@1 40.625 (31.417)\tPrec@5 86.719 (83.357)\n",
      "Epoch: [0][300/391]\tTime 0.663 (0.715)\tData 0.009 (0.010)\tLoss 1.5091 (1.7992)\tPrec@1 45.312 (35.237)\tPrec@5 90.625 (85.878)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/79]\tTime 0.843 (0.843)\tLoss 1.4251 (1.4251)\tPrec@1 51.562 (51.562)\tPrec@5 89.844 (89.844)\n",
      " * Prec@1 47.680 Prec@5 93.090\n",
      "Epoch: [1][0/391]\tTime 3.245 (3.245)\tData 0.452 (0.452)\tLoss 1.5748 (1.5748)\tPrec@1 39.062 (39.062)\tPrec@5 92.969 (92.969)\n",
      "Epoch: [1][100/391]\tTime 0.695 (0.729)\tData 0.009 (0.013)\tLoss 1.2795 (1.3143)\tPrec@1 53.125 (53.125)\tPrec@5 92.969 (93.588)\n",
      "Epoch: [1][200/391]\tTime 0.703 (0.719)\tData 0.008 (0.011)\tLoss 1.1226 (1.2535)\tPrec@1 61.719 (55.162)\tPrec@5 93.750 (94.236)\n",
      "Epoch: [1][300/391]\tTime 0.699 (0.717)\tData 0.013 (0.010)\tLoss 1.0199 (1.1941)\tPrec@1 56.250 (57.309)\tPrec@5 97.656 (94.902)\n",
      "Test: [0/79]\tTime 0.737 (0.737)\tLoss 1.0290 (1.0290)\tPrec@1 66.406 (66.406)\tPrec@5 98.438 (98.438)\n",
      " * Prec@1 63.290 Prec@5 96.190\n",
      "Epoch: [2][0/391]\tTime 1.138 (1.138)\tData 0.356 (0.356)\tLoss 1.0180 (1.0180)\tPrec@1 64.844 (64.844)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [2][100/391]\tTime 0.678 (0.713)\tData 0.008 (0.012)\tLoss 0.9994 (0.9060)\tPrec@1 64.844 (68.301)\tPrec@5 96.875 (97.308)\n",
      "Epoch: [2][200/391]\tTime 0.694 (0.710)\tData 0.008 (0.010)\tLoss 0.8331 (0.8776)\tPrec@1 67.969 (69.018)\tPrec@5 98.438 (97.629)\n",
      "Epoch: [2][300/391]\tTime 0.700 (0.709)\tData 0.008 (0.010)\tLoss 0.7144 (0.8499)\tPrec@1 75.000 (69.978)\tPrec@5 99.219 (97.778)\n",
      "Test: [0/79]\tTime 0.703 (0.703)\tLoss 0.6654 (0.6654)\tPrec@1 77.344 (77.344)\tPrec@5 98.438 (98.438)\n",
      " * Prec@1 70.300 Prec@5 97.820\n",
      "Epoch: [3][0/391]\tTime 1.162 (1.162)\tData 0.398 (0.398)\tLoss 0.8420 (0.8420)\tPrec@1 72.656 (72.656)\tPrec@5 97.656 (97.656)\n",
      "Epoch: [3][100/391]\tTime 0.706 (0.717)\tData 0.014 (0.013)\tLoss 0.7065 (0.6810)\tPrec@1 79.688 (76.423)\tPrec@5 96.875 (98.600)\n",
      "Epoch: [3][200/391]\tTime 0.714 (0.715)\tData 0.008 (0.011)\tLoss 0.7894 (0.6758)\tPrec@1 75.781 (76.741)\tPrec@5 97.656 (98.612)\n",
      "Epoch: [3][300/391]\tTime 0.710 (0.714)\tData 0.008 (0.010)\tLoss 0.7816 (0.6609)\tPrec@1 76.562 (77.248)\tPrec@5 96.875 (98.658)\n",
      "Test: [0/79]\tTime 0.695 (0.695)\tLoss 0.6901 (0.6901)\tPrec@1 73.438 (73.438)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 75.050 Prec@5 98.580\n",
      "Epoch: [4][0/391]\tTime 1.300 (1.300)\tData 0.488 (0.488)\tLoss 0.5540 (0.5540)\tPrec@1 81.250 (81.250)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [4][100/391]\tTime 0.690 (0.712)\tData 0.010 (0.013)\tLoss 0.4724 (0.5363)\tPrec@1 82.031 (81.629)\tPrec@5 100.000 (99.095)\n",
      "Epoch: [4][200/391]\tTime 0.701 (0.709)\tData 0.010 (0.011)\tLoss 0.5432 (0.5291)\tPrec@1 75.781 (81.658)\tPrec@5 99.219 (99.149)\n",
      "Epoch: [4][300/391]\tTime 0.693 (0.709)\tData 0.008 (0.010)\tLoss 0.4870 (0.5278)\tPrec@1 82.031 (81.696)\tPrec@5 99.219 (99.128)\n",
      "Test: [0/79]\tTime 0.685 (0.685)\tLoss 0.6263 (0.6263)\tPrec@1 81.250 (81.250)\tPrec@5 96.875 (96.875)\n",
      " * Prec@1 79.160 Prec@5 98.930\n",
      "Epoch: [5][0/391]\tTime 1.307 (1.307)\tData 0.477 (0.477)\tLoss 0.3469 (0.3469)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [5][100/391]\tTime 0.756 (0.719)\tData 0.009 (0.013)\tLoss 0.3398 (0.4125)\tPrec@1 88.281 (85.961)\tPrec@5 99.219 (99.397)\n",
      "Epoch: [5][200/391]\tTime 0.704 (0.719)\tData 0.008 (0.011)\tLoss 0.5430 (0.4270)\tPrec@1 79.688 (85.148)\tPrec@5 98.438 (99.394)\n",
      "Epoch: [5][300/391]\tTime 0.696 (0.716)\tData 0.008 (0.010)\tLoss 0.3643 (0.4329)\tPrec@1 86.719 (85.016)\tPrec@5 99.219 (99.377)\n",
      "Test: [0/79]\tTime 0.738 (0.738)\tLoss 0.5791 (0.5791)\tPrec@1 78.125 (78.125)\tPrec@5 99.219 (99.219)\n",
      " * Prec@1 78.290 Prec@5 98.680\n",
      "Epoch: [6][0/391]\tTime 1.195 (1.195)\tData 0.456 (0.456)\tLoss 0.3199 (0.3199)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [6][100/391]\tTime 0.758 (0.713)\tData 0.009 (0.013)\tLoss 0.4893 (0.3542)\tPrec@1 81.250 (87.701)\tPrec@5 100.000 (99.652)\n",
      "Epoch: [6][200/391]\tTime 0.700 (0.712)\tData 0.008 (0.011)\tLoss 0.4890 (0.3645)\tPrec@1 82.031 (87.337)\tPrec@5 99.219 (99.592)\n",
      "Epoch: [6][300/391]\tTime 0.703 (0.712)\tData 0.008 (0.010)\tLoss 0.4086 (0.3698)\tPrec@1 85.156 (87.134)\tPrec@5 98.438 (99.598)\n",
      "Test: [0/79]\tTime 0.808 (0.808)\tLoss 0.4035 (0.4035)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 82.520 Prec@5 99.340\n",
      "Epoch: [7][0/391]\tTime 1.179 (1.179)\tData 0.409 (0.409)\tLoss 0.3311 (0.3311)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [7][100/391]\tTime 0.752 (0.714)\tData 0.009 (0.013)\tLoss 0.3107 (0.3030)\tPrec@1 91.406 (89.488)\tPrec@5 99.219 (99.722)\n",
      "Epoch: [7][200/391]\tTime 0.689 (0.712)\tData 0.009 (0.011)\tLoss 0.3381 (0.3060)\tPrec@1 87.500 (89.187)\tPrec@5 100.000 (99.724)\n",
      "Epoch: [7][300/391]\tTime 0.704 (0.712)\tData 0.008 (0.010)\tLoss 0.2175 (0.3113)\tPrec@1 89.844 (89.114)\tPrec@5 100.000 (99.699)\n",
      "Test: [0/79]\tTime 0.745 (0.745)\tLoss 0.4956 (0.4956)\tPrec@1 83.594 (83.594)\tPrec@5 98.438 (98.438)\n",
      " * Prec@1 80.730 Prec@5 99.010\n",
      "Epoch: [8][0/391]\tTime 1.168 (1.168)\tData 0.401 (0.401)\tLoss 0.3635 (0.3635)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [8][100/391]\tTime 0.763 (0.714)\tData 0.010 (0.013)\tLoss 0.2121 (0.2429)\tPrec@1 91.406 (91.453)\tPrec@5 100.000 (99.791)\n",
      "Epoch: [8][200/391]\tTime 0.740 (0.713)\tData 0.009 (0.011)\tLoss 0.3220 (0.2600)\tPrec@1 88.281 (90.998)\tPrec@5 98.438 (99.763)\n",
      "Epoch: [8][300/391]\tTime 0.692 (0.713)\tData 0.008 (0.010)\tLoss 0.2464 (0.2679)\tPrec@1 89.062 (90.822)\tPrec@5 100.000 (99.761)\n",
      "Test: [0/79]\tTime 0.688 (0.688)\tLoss 0.4882 (0.4882)\tPrec@1 84.375 (84.375)\tPrec@5 99.219 (99.219)\n",
      " * Prec@1 84.210 Prec@5 99.330\n",
      "Epoch: [9][0/391]\tTime 1.207 (1.207)\tData 0.477 (0.477)\tLoss 0.2229 (0.2229)\tPrec@1 92.969 (92.969)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [9][100/391]\tTime 0.704 (0.715)\tData 0.008 (0.013)\tLoss 0.4303 (0.2136)\tPrec@1 83.594 (92.466)\tPrec@5 100.000 (99.876)\n",
      "Epoch: [9][200/391]\tTime 0.719 (0.714)\tData 0.009 (0.011)\tLoss 0.3298 (0.2326)\tPrec@1 86.719 (91.950)\tPrec@5 100.000 (99.868)\n",
      "Epoch: [9][300/391]\tTime 0.695 (0.713)\tData 0.008 (0.010)\tLoss 0.3161 (0.2389)\tPrec@1 85.938 (91.705)\tPrec@5 100.000 (99.855)\n",
      "Test: [0/79]\tTime 0.697 (0.697)\tLoss 0.3263 (0.3263)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 83.400 Prec@5 99.450\n",
      "Epoch: [10][0/391]\tTime 1.134 (1.134)\tData 0.432 (0.432)\tLoss 0.2226 (0.2226)\tPrec@1 92.969 (92.969)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [10][100/391]\tTime 0.751 (0.716)\tData 0.010 (0.013)\tLoss 0.0898 (0.1856)\tPrec@1 97.656 (93.301)\tPrec@5 100.000 (99.915)\n",
      "Epoch: [10][200/391]\tTime 0.744 (0.716)\tData 0.010 (0.011)\tLoss 0.2352 (0.1975)\tPrec@1 90.625 (93.008)\tPrec@5 100.000 (99.922)\n",
      "Epoch: [10][300/391]\tTime 0.698 (0.716)\tData 0.008 (0.010)\tLoss 0.3253 (0.2017)\tPrec@1 89.062 (92.922)\tPrec@5 99.219 (99.901)\n",
      "Test: [0/79]\tTime 0.844 (0.844)\tLoss 0.2847 (0.2847)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 86.140 Prec@5 99.390\n",
      "Epoch: [11][0/391]\tTime 1.238 (1.238)\tData 0.553 (0.553)\tLoss 0.1331 (0.1331)\tPrec@1 94.531 (94.531)\tPrec@5 99.219 (99.219)\n",
      "Epoch: [11][100/391]\tTime 0.727 (0.712)\tData 0.008 (0.014)\tLoss 0.1105 (0.1551)\tPrec@1 96.094 (94.562)\tPrec@5 100.000 (99.938)\n",
      "Epoch: [11][200/391]\tTime 0.768 (0.712)\tData 0.014 (0.011)\tLoss 0.1825 (0.1625)\tPrec@1 94.531 (94.240)\tPrec@5 100.000 (99.942)\n",
      "Epoch: [11][300/391]\tTime 0.747 (0.712)\tData 0.008 (0.011)\tLoss 0.2544 (0.1701)\tPrec@1 90.625 (93.958)\tPrec@5 100.000 (99.938)\n",
      "Test: [0/79]\tTime 0.727 (0.727)\tLoss 0.4407 (0.4407)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 85.690 Prec@5 99.440\n",
      "Epoch: [12][0/391]\tTime 1.154 (1.154)\tData 0.394 (0.394)\tLoss 0.1054 (0.1054)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [12][100/391]\tTime 0.733 (0.715)\tData 0.008 (0.012)\tLoss 0.1792 (0.1460)\tPrec@1 93.750 (95.096)\tPrec@5 100.000 (99.938)\n",
      "Epoch: [12][200/391]\tTime 0.748 (0.714)\tData 0.008 (0.010)\tLoss 0.2168 (0.1464)\tPrec@1 91.406 (94.955)\tPrec@5 99.219 (99.934)\n",
      "Epoch: [12][300/391]\tTime 0.715 (0.713)\tData 0.008 (0.010)\tLoss 0.1852 (0.1551)\tPrec@1 92.188 (94.601)\tPrec@5 100.000 (99.940)\n",
      "Test: [0/79]\tTime 0.687 (0.687)\tLoss 0.3444 (0.3444)\tPrec@1 89.062 (89.062)\tPrec@5 98.438 (98.438)\n",
      " * Prec@1 86.400 Prec@5 99.210\n",
      "Epoch: [13][0/391]\tTime 1.209 (1.209)\tData 0.455 (0.455)\tLoss 0.0858 (0.0858)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [13][100/391]\tTime 0.682 (0.715)\tData 0.008 (0.013)\tLoss 0.0881 (0.1120)\tPrec@1 95.312 (96.001)\tPrec@5 100.000 (99.985)\n",
      "Epoch: [13][200/391]\tTime 0.735 (0.714)\tData 0.008 (0.011)\tLoss 0.1493 (0.1203)\tPrec@1 93.750 (95.666)\tPrec@5 100.000 (99.973)\n",
      "Epoch: [13][300/391]\tTime 0.740 (0.714)\tData 0.008 (0.010)\tLoss 0.1655 (0.1279)\tPrec@1 92.969 (95.434)\tPrec@5 100.000 (99.969)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/79]\tTime 0.731 (0.731)\tLoss 0.5012 (0.5012)\tPrec@1 86.719 (86.719)\tPrec@5 98.438 (98.438)\n",
      " * Prec@1 85.390 Prec@5 99.290\n",
      "Epoch: [14][0/391]\tTime 1.137 (1.137)\tData 0.372 (0.372)\tLoss 0.0865 (0.0865)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [14][100/391]\tTime 0.691 (0.714)\tData 0.009 (0.012)\tLoss 0.0869 (0.1074)\tPrec@1 95.312 (96.101)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [14][200/391]\tTime 0.735 (0.713)\tData 0.008 (0.011)\tLoss 0.1345 (0.1056)\tPrec@1 96.094 (96.323)\tPrec@5 100.000 (99.988)\n",
      "Epoch: [14][300/391]\tTime 0.741 (0.712)\tData 0.009 (0.010)\tLoss 0.1214 (0.1141)\tPrec@1 95.312 (95.974)\tPrec@5 99.219 (99.977)\n",
      "Test: [0/79]\tTime 0.651 (0.651)\tLoss 0.3767 (0.3767)\tPrec@1 88.281 (88.281)\tPrec@5 99.219 (99.219)\n",
      " * Prec@1 84.670 Prec@5 99.430\n",
      "Epoch: [15][0/391]\tTime 1.136 (1.136)\tData 0.426 (0.426)\tLoss 0.0547 (0.0547)\tPrec@1 98.438 (98.438)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [15][100/391]\tTime 0.716 (0.715)\tData 0.010 (0.013)\tLoss 0.0776 (0.0974)\tPrec@1 96.875 (96.573)\tPrec@5 100.000 (99.992)\n",
      "Epoch: [15][200/391]\tTime 0.734 (0.714)\tData 0.009 (0.011)\tLoss 0.0775 (0.0995)\tPrec@1 97.656 (96.502)\tPrec@5 100.000 (99.992)\n",
      "Epoch: [15][300/391]\tTime 0.755 (0.715)\tData 0.008 (0.010)\tLoss 0.0918 (0.1010)\tPrec@1 97.656 (96.480)\tPrec@5 100.000 (99.990)\n",
      "Test: [0/79]\tTime 0.664 (0.664)\tLoss 0.4941 (0.4941)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 85.280 Prec@5 99.300\n",
      "Epoch: [16][0/391]\tTime 1.109 (1.109)\tData 0.408 (0.408)\tLoss 0.0345 (0.0345)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [16][100/391]\tTime 0.701 (0.714)\tData 0.008 (0.013)\tLoss 0.0841 (0.0919)\tPrec@1 96.094 (96.860)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [16][200/391]\tTime 0.684 (0.713)\tData 0.008 (0.011)\tLoss 0.0752 (0.0875)\tPrec@1 97.656 (97.027)\tPrec@5 100.000 (99.996)\n",
      "Epoch: [16][300/391]\tTime 0.733 (0.714)\tData 0.010 (0.010)\tLoss 0.0786 (0.0918)\tPrec@1 96.875 (96.813)\tPrec@5 100.000 (99.997)\n",
      "Test: [0/79]\tTime 0.788 (0.788)\tLoss 0.3051 (0.3051)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      " * Prec@1 87.610 Prec@5 99.610\n",
      "Epoch: [17][0/391]\tTime 1.101 (1.101)\tData 0.382 (0.382)\tLoss 0.0649 (0.0649)\tPrec@1 97.656 (97.656)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [17][100/391]\tTime 0.708 (0.719)\tData 0.008 (0.012)\tLoss 0.1352 (0.0807)\tPrec@1 95.312 (97.231)\tPrec@5 100.000 (99.992)\n",
      "Epoch: [17][200/391]\tTime 0.746 (0.719)\tData 0.014 (0.011)\tLoss 0.0728 (0.0827)\tPrec@1 98.438 (97.167)\tPrec@5 100.000 (99.992)\n",
      "Epoch: [17][300/391]\tTime 0.712 (0.717)\tData 0.009 (0.010)\tLoss 0.0836 (0.0826)\tPrec@1 96.875 (97.148)\tPrec@5 100.000 (99.995)\n",
      "Test: [0/79]\tTime 0.655 (0.655)\tLoss 0.3282 (0.3282)\tPrec@1 86.719 (86.719)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 86.930 Prec@5 99.470\n",
      "Epoch: [18][0/391]\tTime 1.094 (1.094)\tData 0.385 (0.385)\tLoss 0.1787 (0.1787)\tPrec@1 94.531 (94.531)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [18][100/391]\tTime 0.703 (0.711)\tData 0.008 (0.012)\tLoss 0.0336 (0.0719)\tPrec@1 99.219 (97.633)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [18][200/391]\tTime 0.707 (0.713)\tData 0.012 (0.011)\tLoss 0.0642 (0.0707)\tPrec@1 98.438 (97.621)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [18][300/391]\tTime 0.753 (0.714)\tData 0.009 (0.010)\tLoss 0.1078 (0.0751)\tPrec@1 96.875 (97.425)\tPrec@5 100.000 (99.995)\n",
      "Test: [0/79]\tTime 0.742 (0.742)\tLoss 0.3720 (0.3720)\tPrec@1 88.281 (88.281)\tPrec@5 100.000 (100.000)\n",
      " * Prec@1 87.640 Prec@5 99.520\n",
      "Epoch: [19][0/391]\tTime 1.105 (1.105)\tData 0.381 (0.381)\tLoss 0.0700 (0.0700)\tPrec@1 97.656 (97.656)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [19][100/391]\tTime 0.746 (0.720)\tData 0.008 (0.012)\tLoss 0.0712 (0.0620)\tPrec@1 97.656 (97.919)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [19][200/391]\tTime 0.670 (0.717)\tData 0.009 (0.011)\tLoss 0.0391 (0.0576)\tPrec@1 98.438 (98.045)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [19][300/391]\tTime 0.690 (0.716)\tData 0.008 (0.010)\tLoss 0.0391 (0.0602)\tPrec@1 99.219 (97.929)\tPrec@5 100.000 (100.000)\n",
      "Test: [0/79]\tTime 0.632 (0.632)\tLoss 0.4019 (0.4019)\tPrec@1 89.062 (89.062)\tPrec@5 99.219 (99.219)\n",
      " * Prec@1 88.340 Prec@5 99.440\n",
      "Epoch: [20][0/391]\tTime 1.167 (1.167)\tData 0.359 (0.359)\tLoss 0.1217 (0.1217)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [20][100/391]\tTime 0.747 (0.719)\tData 0.008 (0.012)\tLoss 0.0856 (0.0486)\tPrec@1 97.656 (98.368)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [20][200/391]\tTime 0.713 (0.717)\tData 0.009 (0.010)\tLoss 0.0623 (0.0514)\tPrec@1 98.438 (98.278)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [20][300/391]\tTime 0.673 (0.718)\tData 0.008 (0.010)\tLoss 0.0326 (0.0547)\tPrec@1 100.000 (98.134)\tPrec@5 100.000 (100.000)\n",
      "Test: [0/79]\tTime 0.636 (0.636)\tLoss 0.2860 (0.2860)\tPrec@1 92.188 (92.188)\tPrec@5 99.219 (99.219)\n",
      " * Prec@1 88.540 Prec@5 99.550\n",
      "Epoch: [21][0/391]\tTime 1.074 (1.074)\tData 0.370 (0.370)\tLoss 0.0361 (0.0361)\tPrec@1 99.219 (99.219)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [21][100/391]\tTime 0.697 (0.715)\tData 0.010 (0.012)\tLoss 0.0713 (0.0495)\tPrec@1 96.875 (98.252)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [21][200/391]\tTime 0.677 (0.713)\tData 0.009 (0.011)\tLoss 0.0156 (0.0507)\tPrec@1 99.219 (98.208)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [21][300/391]\tTime 0.712 (0.713)\tData 0.010 (0.010)\tLoss 0.0607 (0.0529)\tPrec@1 97.656 (98.170)\tPrec@5 100.000 (99.997)\n",
      "Test: [0/79]\tTime 0.710 (0.710)\tLoss 0.4333 (0.4333)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      " * Prec@1 87.750 Prec@5 99.540\n",
      "Epoch: [22][0/391]\tTime 1.049 (1.049)\tData 0.394 (0.394)\tLoss 0.0651 (0.0651)\tPrec@1 97.656 (97.656)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [22][100/391]\tTime 0.759 (0.717)\tData 0.008 (0.012)\tLoss 0.0154 (0.0429)\tPrec@1 100.000 (98.453)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [22][200/391]\tTime 0.707 (0.717)\tData 0.010 (0.010)\tLoss 0.0259 (0.0474)\tPrec@1 99.219 (98.340)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [22][300/391]\tTime 0.697 (0.717)\tData 0.009 (0.010)\tLoss 0.0394 (0.0513)\tPrec@1 99.219 (98.147)\tPrec@5 100.000 (100.000)\n",
      "Test: [0/79]\tTime 0.671 (0.671)\tLoss 0.2894 (0.2894)\tPrec@1 89.844 (89.844)\tPrec@5 99.219 (99.219)\n",
      " * Prec@1 88.140 Prec@5 99.440\n",
      "Epoch: [23][0/391]\tTime 1.294 (1.294)\tData 0.511 (0.511)\tLoss 0.0577 (0.0577)\tPrec@1 97.656 (97.656)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [23][100/391]\tTime 0.684 (0.715)\tData 0.008 (0.014)\tLoss 0.0400 (0.0470)\tPrec@1 98.438 (98.515)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [23][200/391]\tTime 0.712 (0.715)\tData 0.009 (0.011)\tLoss 0.0518 (0.0501)\tPrec@1 98.438 (98.270)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [23][300/391]\tTime 0.698 (0.715)\tData 0.008 (0.010)\tLoss 0.0855 (0.0513)\tPrec@1 97.656 (98.230)\tPrec@5 100.000 (100.000)\n",
      "Test: [0/79]\tTime 0.726 (0.726)\tLoss 0.2778 (0.2778)\tPrec@1 93.750 (93.750)\tPrec@5 99.219 (99.219)\n",
      " * Prec@1 88.690 Prec@5 99.380\n",
      "Epoch: [24][0/391]\tTime 1.134 (1.134)\tData 0.370 (0.370)\tLoss 0.0324 (0.0324)\tPrec@1 99.219 (99.219)\tPrec@5 100.000 (100.000)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-591bd42b8cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 1 epoch training, validation 을 함수로 따로 정의함. => 코드가 훨씬 깔끔.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0cbd6a5b0971>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, lr_scheduler, epoch, writer)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36m_worker\u001b[0;34m(i, module, input, kwargs, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/kyy/RWNN_NSGA_kyy/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/kyy/RWNN_NSGA_kyy/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# 따라서, input으로 넣을 때 unpack 함.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# id 작은 노드부터 result를 차근차근 계산하면서, id를 올라감.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodeop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/kyy/RWNN_NSGA_kyy/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/kyy/RWNN_NSGA_kyy/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# 5. Train model\n",
    "####################################################\n",
    "niters = len(train_loader)\n",
    "\n",
    "lr_scheduler = LRScheduler(optimizer, niters, args)  # (default) args.step = [30, 60, 90], args.decay_factor = 0.1, args.power = 2.0\n",
    "\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "    # train for one epoch\n",
    "    ####################################################\n",
    "    # 1 epoch training, validation 을 함수로 따로 정의함. => 코드가 훨씬 깔끔.\n",
    "    ####################################################        \n",
    "    train(train_loader, model, criterion, optimizer, lr_scheduler, epoch, writer)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(val_loader, model, criterion, epoch, writer)\n",
    "\n",
    "    #  write log\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    \n",
    "    ########################################################## save checkpoint 구현해놔야함 ###################3\n",
    "    # is_best 일 때, save checkpoint\n",
    "    # run code에 모델내 그래프는 있으니까,\n",
    "    # epoch, model.state_dict(), best_prec1, optimizer.state_dict() 만 저장하면 될 듯\n",
    "\n",
    "#     save_checkpoint(model_dir, {\n",
    "#         'epoch': epoch + 1,\n",
    "#         'model': args.config.rsplit('/',1)[-1].split('.yaml')[0],\n",
    "#         'state_dict': model.state_dict(),\n",
    "#         'best_prec1': best_prec1,\n",
    "#         'optimizer': optimizer.state_dict(),\n",
    "#     }, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# def save_checkpoint(model_dir, state, is_best):\n",
    "#     epoch = state['epoch']\n",
    "#     path = os.path.join(model_dir, 'model.pth-' + str(epoch))\n",
    "#     torch.save(state, path)\n",
    "#     checkpoint_file = os.path.join(model_dir, 'checkpoint')\n",
    "#     checkpoint = open(checkpoint_file, 'w+')\n",
    "#     checkpoint.write('model_checkpoint_path:%s\\n' % path)\n",
    "#     checkpoint.close()\n",
    "#     if is_best:\n",
    "#         shutil.copyfile(path, os.path.join(model_dir, 'model-best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Distributed 패키지 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "ngpus_per_node = torch.cuda.device_count()\n",
    "print(ngpus_per_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
